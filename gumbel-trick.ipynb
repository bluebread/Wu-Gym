{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "7a3ed771-44d8-4ef3-9ed6-975d9dc90156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "194ba8e0-f137-4744-a3ea-73a2c2e89c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsetSampler(torch.nn.Module):\n",
    "    def __init__(self, k, tau=1.0, hard=False):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.hard = hard\n",
    "        self.tau = tau\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.softmax(x, dim=-1)\n",
    "        n = x.shape[-1]\n",
    "        gumbel = torch.distributions.gumbel.Gumbel(torch.zeros_like(x), torch.ones_like(x))\n",
    "        x = torch.log(x) + gumbel.sample()\n",
    "\n",
    "        \n",
    "        if self.hard:\n",
    "            values, indices = torch.topk(x, self.k)\n",
    "            return indices\n",
    "        else:\n",
    "            values, indices = torch.topk(x, self.k)\n",
    "            khot_prime = torch.nn.functional.one_hot(indices, num_classes=n).sum(dim=0)\n",
    "            \n",
    "            # Top-k relaxation\n",
    "            y = torch.zeros_like(x)\n",
    "            khot = torch.zeros_like(x)\n",
    "            \n",
    "            for i in range(k):\n",
    "                y = torch.nn.functional.softmax(x, dim=-1)\n",
    "                khot += y\n",
    "                x = x + torch.log(1 - y)\n",
    "\n",
    "            khot = torch.nn.functional.softmax(khot, dim=-1) * k\n",
    "            \n",
    "            return khot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "e0b6b900-e7d2-4a8e-a84f-ae97558d1f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250])\n"
     ]
    }
   ],
   "source": [
    "# Test top-k relaxation\n",
    "k = 3\n",
    "sampler = SubsetSampler(k=k, tau=1.0, hard=False)\n",
    "n = 8\n",
    "x = torch.ones(n)\n",
    "y = sampler(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "fc045964-bfb4-41db-90b9-82d3f8d770f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability error: tensor([-0.0270, -0.0040,  0.0210, -0.0020, -0.0110,  0.0050, -0.0110,  0.0290])\n"
     ]
    }
   ],
   "source": [
    "# Test uniform distribution over subsets of size 3\n",
    "k = 3\n",
    "sampler = SubsetSampler(k=k, tau=1.0, hard=True)\n",
    "n = 8\n",
    "x = torch.ones(n)\n",
    "y = torch.zeros_like(x)\n",
    "\n",
    "for i in range(1000):\n",
    "    subset = sampler(x)\n",
    "    khot = torch.nn.functional.one_hot(subset, num_classes=n).sum(dim=0)\n",
    "    y += khot\n",
    "\n",
    "z = y / 1000 - (k/n)\n",
    "print(f\"probability error: {z}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "add9c691-80de-4865-92dc-882ddabbe8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: 0.2535242736339569\n",
      "statistic: 0.254265\n"
     ]
    }
   ],
   "source": [
    "# Test subset's probability\n",
    "n = 8\n",
    "k = 3\n",
    "m = 1000000\n",
    "x = torch.tensor([1,2,3,4,5,6,7,8], dtype=torch.float)\n",
    "sampler = SubsetSampler(k=k, tau=1.0, hard=True)\n",
    "\n",
    "probs = torch.nn.functional.softmax(x, dim=-1)\n",
    "target = [7,6,5]\n",
    "count = 0\n",
    "\n",
    "for i in range(m):\n",
    "    indices = sampler(x).tolist()\n",
    "    if indices == target:\n",
    "        count += 1\n",
    "\n",
    "predicted_p = probs[-1] * (probs[-2] / (1 - probs[-1])) * (probs[-3] / (1 - probs[-1] - probs[-2]))\n",
    "real_p = count / m\n",
    "\n",
    "print(f\"predict: {predicted_p}\")\n",
    "print(f\"statistic: {real_p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "7c304648-bc16-4a1c-8797-afe817bc45ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_prob(S: tuple, a: torch.tensor):\n",
    "    k = len(S)\n",
    "    n = a.shape[0]\n",
    "    a = torch.nn.functional.softmax(a, dim=-1)\n",
    "    A = a[list(S)].prod()\n",
    "    Z = torch.zeros(k)\n",
    "\n",
    "    # dynamic programming: E[i,j] = E[i-1,j] + a[i] * E[i-1,j-1]\n",
    "    for i in range(n):\n",
    "        shift_Z = torch.cat([Z.new_ones(1), Z[:-1]])\n",
    "        Z = Z + a[i] * shift_Z\n",
    "\n",
    "    return A / Z[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "id": "3a51658d-9f6a-4c85-b4f9-d48fb2947288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob.: 0.01785714365541935\n",
      "prob. error: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Test uniform distribution over size-k subsets\n",
    "k = 3\n",
    "n = 8\n",
    "a = torch.ones(8, requires_grad=True)\n",
    "S = (1,2,3)\n",
    "p = subset_prob(S, a)\n",
    "# print(p.requires_grad)\n",
    "print(f\"prob.: {p}\")\n",
    "print(f\"prob. error: {torch.abs((1/math.comb(n,k)) - p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "id": "50b698d7-8ede-4ca9-bac9-3b68ff8d9e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def subset_prob_seq(S: tuple, a: torch.Tensor):\n",
    "    # Sequential form: sum over permutations\n",
    "    a = torch.nn.functional.softmax(a, dim=-1)\n",
    "    A = 0.0\n",
    "    for perm in itertools.permutations(S):\n",
    "        prob = 1.0\n",
    "        remaining = list(range(len(a)))\n",
    "        for idx in perm:\n",
    "            prob *= a[idx].item() / sum(a[remaining]).item()\n",
    "            remaining.remove(idx)\n",
    "        A += prob\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "9c0fb03b-ba0b-4960-84cd-ca9de75d3072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target subset: [5, 6, 7]\n",
      "ans: tensor(0.5894)\n",
      "P_seq:  0.5894127898446275\n",
      "P_dp: 0.5213007926940918\n",
      "SubsetSampler (10000 times): 0.5828\n"
     ]
    }
   ],
   "source": [
    "n = 8\n",
    "k = 3\n",
    "m = 10000\n",
    "# x = torch.tensor([1] * n, dtype=torch.float)\n",
    "x = torch.tensor([1,2,3,4,5,6,7,8], dtype=torch.float)\n",
    "sampler = SubsetSampler(k=k, tau=1.0, hard=True)\n",
    "target = [5,6,7]\n",
    "count = 0\n",
    "for i in range(m):\n",
    "    indices = sampler(x).tolist()\n",
    "    indices.sort()\n",
    "    if indices == target:\n",
    "        count += 1\n",
    "\n",
    "prob_predict = subset_prob(target, x)\n",
    "l = subset_prob_seq(target, x)\n",
    "\n",
    "l1 = probs[-1] * (probs[-2] / (1 - probs[-1])) * (probs[-3] / (1 - probs[-1] - probs[-2]))\n",
    "l2 = probs[-1] * (probs[-3] / (1 - probs[-1])) * (probs[-2] / (1 - probs[-1] - probs[-3]))\n",
    "l3 = probs[-2] * (probs[-1] / (1 - probs[-2])) * (probs[-3] / (1 - probs[-2] - probs[-1]))\n",
    "l4 = probs[-2] * (probs[-3] / (1 - probs[-2])) * (probs[-1] / (1 - probs[-2] - probs[-3]))\n",
    "l5 = probs[-3] * (probs[-1] / (1 - probs[-3])) * (probs[-2] / (1 - probs[-3] - probs[-1]))\n",
    "l6 = probs[-3] * (probs[-2] / (1 - probs[-3])) * (probs[-1] / (1 - probs[-3] - probs[-2]))\n",
    "\n",
    "\n",
    "print(f\"target subset: {target}\")\n",
    "print(\"ans:\", l1 + l2 + l3 + l4 + l5 + l6)\n",
    "print(\"P_seq: \", l)\n",
    "print(f\"P_dp: {prob_predict}\")\n",
    "print(f\"SubsetSampler ({m} times): {count / m}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "id": "af841a4c-e335-4d71-bf3a-70ce62d59e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion: P_seq is correct and P_dp is NOT equivalent to P_seq\n",
    "# TODO: ChatGPT claims that P_seq can be calculated in O(k * 2^k) time using DP technique"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
